##### 通用爬虫 CrawlSpiders



https://blog.csdn.net/wqh_jingsong/article/details/56865433



通过下面的命令可以快速创建 CrawlSpider模板 的代码： 

> scrapy genspider -t crawl tencent tencent.com 

它是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取的工作更适合。 

```python
# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from scrapy01.items import ScrapydyItem
import re

class DySpider(CrawlSpider):
    name = 'dy'
    allowed_domains = ['www.dy2018.com']
    start_urls = ['https://www.dy2018.com/']
	
    # 这里定义了一些URL跟进的规则
    rules = (
        Rule(LinkExtractor(restrict_xpaths="//div[@id='menu']/div/ul/li[position()<=11]/a"),
             callback='parse_item', follow=False),
    )

    # 这里是设置的回调函数
   
    def parse_item(self, response):
        detailURLs = response.xpath('//div[@class="co_content8"]/ul/td/table/tr/td/b/a[last()]/@href')
        # print(response.urljoin(detailURL))
        for item in detailURLs.extract():
            detail = response.urljoin(item)
            yield scrapy.Request(url=detail, callback=self.parse_detail)
```



##### follow 用途

 如果 follow 为False，那么只会把start_urls 里面的爬取后然后，按照规则解析，得到新的URL然后再发起新的请求，只解析一层。

如果 follow 为 True，那么会把 start_urls 里面的，以及里面里面的都使用这个规则解析，并自动爬取。



简要说明

CrawlSpider是爬取那些具有一定规则网站的常用的爬虫，它基于Spider并有一些独特属性

rules: 是Rule对象的集合，用于匹配目标网站并排除干扰 
parse_start_url: 用于爬取起始响应，必须要返回Item，Request中的一个。 
因为rules是Rule对象的集合，所以这里也要介绍一下Rule。它有几个参数：link_extractor、callback=None、cb_kwargs=None、follow=None、process_links=None、process_request=None 
其中的link_extractor既可以自己定义，也可以使用已有LinkExtractor类，主要参数为：

allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 
deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。 
allow_domains：会被提取的链接的domains。 
deny_domains：一定不会被提取链接的domains。 
restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。还有一个类似的restrict_css 