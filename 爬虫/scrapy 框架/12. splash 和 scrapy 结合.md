#### scrpay-splash 库

有的页面的很多部分都是用 JS 生成的，而对于用 scrapy 爬虫来说就是一个很大的问题，因为 scrapy 没有 JS engine，所以爬取的都是静态页面，对于 JS 生成的动态页面都无法获得

-  利用第三方中间件来提供 JS 渲染服务： scrapy-splash 等
- 利用 webkit 或者基于 webkit 库

spalsh 是一个 JavaScript 渲染服务它是一个实现了 HTTP API的轻量级浏览器，Splash 是用 Python 实现的，同时使用 Twisted 和QT。Twisted（QT）用来让服务具有异步处理能力，以发挥 webkit 的并发能力



#### 安装与配置

###### pip 安装 scrapy-splash 库

`pip install scrapy-splash`

配置 splash 服务（以下操作全部在 settings.py）

- 使用 splash 解析，需要我们在配置文件中设置 spalsh 服务器地址

需要启动这个服务

```python
SPLASH_URL = 'http://localhost:8050'
```

- 将 splash middleware 添加到 DOWNLOADER_MIDDLEWARE 中

```python
DOWNLOADER_MIDDLEWARES = {
'scrapy_splash.SplashCookiesMiddleware': 723,
'scrapy_splash.SplashMiddleware': 725,
'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810, 
}
```

- 然后在 SPIDER_MIDDLEWARES 中也设置这个中间件

```python
SPIDER_MIDDLEWARES = { 
    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100 
}

```

这个中间件需要支持 cache_args 功能; 它允许通过不在磁盘请求队列中多次存储重复的 Splash 参数来节省磁盘空间。

如果使用Splash 2.1+，则中间件也可以通过不将这些重复的参数多次发送到Splash 服务器来节省网络流量

- 配置消息队列所使用的过滤类

```python
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
```

- 配置消息队列需要使用的类

```python
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
```

##### 例子

下面就是使用 splash 发起请求，这样的话，就可以接受解析后的数据

```python
import scrapy from scrapy_splash import SplashRequest
class DoubanSpider(scrapy.Spider): name = 'douban'
allowed_domains = ['douban.com']

def start_requests(self):
    splash_headers = {"User_Agent":"User-Agent:Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36"}
	
    # 使用 splash 发起请求  自己在测试的时候返回的结果就是解析后的 不需要设置 wait
    yield SplashRequest('https://movie.douban.com/typerank?type_name=剧情&type=11&interval_id=100:90', args={'wait': 0.5,'headers': splash_headers})
    
	def parse(self, response): 
        print(response.text)
    
```

#### 自己利用中间件实现

在 setting 里面设置

```python
DOWNLOADER_MIDDLEWARES = {
    'scrapy02.middlewares.testMiddle': 900,
}
```

编写中间件

```python
from scrapy.http import HtmlResponse
class testMiddle:
    def process_request(self, request, spider):
        url = 'http://192.168.186.128:8050/render.html?url={}'.format(request.url)
        re = requests.get(url)
        
        # 这里就是返回响应了 然后其他地方都会接到解析后的数据
        return HtmlResponse(url=request.url, body=re.text, encoding='utf8')
```

