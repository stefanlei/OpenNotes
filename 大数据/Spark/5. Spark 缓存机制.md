### 缓存机制

当我们需要多次使用一个一个RDD，就可以使用缓存机制。

Spark RDD 是惰性求值的，而有时我们希望能多次使用同一个 RDD。如果简单 地对 RDD 调用行动操作，Spark 每次都会重算 RDD 以及它的所有依赖。这在迭代算法中 消耗格外大，因为迭代算法常常会多次使用同一组数据，例如：

```python
from pyspark import SparkConf, SparkContext, StorageLevel
import time

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)

myfile = sc.textFile(r'd:\aipai.sql')

# 这里的map操作，读取一次文件，然后filter操作根据依赖关闭，会执行map计算，然后再读取一次文件。
# 这样多次读取一个RDD就造成了巨大开销。我们可以对某个RDD使用缓存，在第二次读取RDD时候，就从缓存读取。
myfile.map(func),filter(fucn)
```



==默认是不缓存的==

我们可以使用 `cache()` 把数据集缓存到内存中去，然后第二次读取相同数据集的时候，就会从内存中读取。

`cache()` 也是惰性的，意味着第一次调用了 `action`算子后，才会对 RDD 进行缓存，第二次对这个 RDD 使用 Action 算子的时候，就会从内存读取

==从 Driver 端的输出日志中可以看到缓存==

```python
from pyspark import SparkConf, SparkContext, StorageLevel
import time

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)

myfile = sc.textFile(r'd:\aipai.sql')
# 这里设置了缓存后，第二次对这个 RDD 使用 Action 算子的时候，就会从内存读取
myfile.cache()

start = time.clock()
# 这是第一次
myfile.count()
end = time.clock()
print(end - start)

start = time.clock()
# 这里就是第二次，这里会从内存加载数据
myfile.count()
end = time.clock()
print(end - start)

```



#### 持久化级别

调用 cache() 将会在 executor 的内存中持久化保存 RDD 的每个分区，如果 exector 没有足够的内存来储存RDD 分区，计算并不会失败，只不过是根据需要 重新计算分区，而不使用缓存

对于包含大量转换操作的复杂程序来说，重新技术的代价高，因此 spark 提供了不同级别的持久化行为，我们可以通过调用 persist() 来指定 StorageLevel 参数来做出选择

`cache()`默认的持久化级别是 `MEMORY_ONLY`，它使用对象在内存中的常规表示方法。另一种更紧凑的表示方法是通过把分区中的元素序列化为字节数组来实现。这一级别称为`MEMORY_ONLY_SER`。

与 `MEMORY_ONLY`相比，`MEMORY_ONLY_SER`多了一份cpu开销，但是如果它生成的序列化 RDD 分区大小(字节数组比对象小)适合保存到内存中，而常规的表示方法无法做到这一时，这份额外的开销的值得的。

`MEMORY_ONLY_SER`还能减少垃圾回收的压力，因为每个RDD被存为一个字节数组，而不是大量的对象。



如果重新计算数据集的代价太过高昂，那么可以使用 `MEMORY_AND_DISK`(如果数据集的大小不适合保存到内存中，就将其溢写到磁盘)。或者 `MEMORY_AND_DISK_SER`（如果序列化数据集的大小不适合保存在内存，就将其溢写到磁盘）