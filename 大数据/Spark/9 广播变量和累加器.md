### 共享变量

#### 累加器

累加器，提供了将工作节点中的值聚合到驱动器程序中的简单语法。 就是说，提供了一个在所有的工作节点中的共享变量。常见的用途是在调试的时候，对作业执行过程中进行计数的操作

```python
from pyspark import SparkConf, SparkContext
conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)
data = [1, 2, 3, 4, 5, 6, 7]

# 这里就是定义了一个累加器,累加器不 仅仅是可以是整型，也可以是其他类型
n = sc.accumulator(0)

def test(line):
    global n
    if line > 1:
        # 这里一定要这样写，不能 使用 n=n+1
        n += 1
    return line
rdd = sc.parallelize(data)
# 累加器也是惰性的，只有在行动算子执行后，才会运算
rdd.map(test).collect()
print(n)
print(n.value)

```

例子 2

```python
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)

sum = sc.accumulator(0)
RDD = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9])


def test(x):
    global sum
    sum += x
    return x


RDD.map(test).collect()
print(sum)

```

---



#### 广播变量

spark的第二种共享变量类型是广播变量，它可以让程序高效地向所有的工作节点发送一个较大的只读值，以供一个或多个spark操作使用。

使用了广播变量以后，意味着，第二次要用到这个变量里面数据的时候，直接从工作节点本身读取。driver不再发送数据。

广播变量只会发送到工作节点一次

---

前面提过，Spark 会自动把闭包中所有引用到的变量发送到工作节点上。虽然这很方便， 但也很低效。

假如有这样的程序，在运行的时候，spark会自动把test函数使用到的变量a，发到工作节点。假如后面还有函数用到了a变量，那么就再发一次，如果a数据量好大，那么这个网络开销就好大。广播变量的话，就是把数据发送到节点一次，后面如果需要这个变量，那么就直接在节点上的 `BlockManager`取值。

```python
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName('wordcount').setMaster('local')
sc = SparkContext(conf=conf)

# 这里就是定义了一个广播变量

a = sc.broadcast(1233)

# 这里就会把这个变量发到工作节点上去，非广播变量也会被发到工作节点上。
# 第二次如果还要使用到这个变量的时候，那么久直接从工作节点的blockmanager读取。
def test(line):
    # 使用value属性，获取广播变量的值
    return line, a.value

RDD = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9])

result = RDD.map(test)
print(result.collect())

```

==广播变量的注意事项==

-  变量只会被发到各个节点一次，应作为只读值处理（修改这个值不会影响到别的节点）
- 有时候需要自己维护变量的只读的条件。

- 不能将RDD设置为广播变量，但可以把RDD的结果广播出去
- 广播变量只能在Driver端定义，不能在executor端定义（就是说不能在工作节点的函数里面定义），比如不能在上面的test函数里面定义，因为test函数是work端执行的代码。
- 广播变量在driver端可以修改，在executor端不能修改。