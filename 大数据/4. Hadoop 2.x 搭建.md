Web UI 

HDFS 部署了 NameNode 的服务器 50070 端口

Yarn 部署了 ResrouceManager 的服务器 8088 端口

### 搭建 Hadoop 2.x HA 

我一共有 4 台机器，hadoop1, hadoop2, hadoop3, hadoop4

|                                  | hadoop1 | hadoop2 | hadoop3 | hadoop4 |
| :------------------------------: | :-----: | :-----: | :-----: | :-----: |
|             NameNode             |    1    |    1    |         |         |
|             DataNode             |         |    1    |    1    |    1    |
|       Zookeeper (单独启动)       |    1    |    1    |    1    |         |
|    ZKFC （自动跟随 NameNode）    |    1    |    1    |         |         |
|           Journal Node           |         |    1    |    1    |    1    |
|    Resouce Manager (单独启动)    |         |         |    1    |    1    |
| Node Manager (自动跟随 DataNode) |         |    1    |    1    |    1    |
|                                  |         |         |         |         |
|                                  |         |         |         |         |



##### 配置安装 HDFS 2.x 

==所有的服务器上都要有 Hadoop==

##### core-site.xml 配置

```xml
<configuration>
    <property>
        // 这里是配置的是 NameNode 地址  下面的 namenodes 是一个别名，后面需要具体指定 
        // namenode1 namenode2
        <name>fs.defaultFS</name>
        <value>hdfs://namenodes</value>
    </property>
    <property>
        // 这里配置的是 zookeeper 的地址，三个服务器
        <name>ha.zookeeper.quorum</name>
        <value>hadoop1:2181,hadoop2:2181,hadoop3:2181</value>
    </property>
    <property>
        // 这里指定的是 Hadoop 的临时数据目录
        <name>hadoop.tmp.dir</name>
        <value>/opt/hadoop</value>
    </property>
</configuration>
```

##### hdfs-site.xml

```xml
<configuration>
    <property>
        <!--指定hdfs的nameservice为namenodes，需要和core-site.xml中的保持一致 -->
        <name>dfs.nameservices</name>
        <value>namenodes</value>
    </property>
    <property>
        <!--namenodes 下面有两个 namecode 我们这里配置成 namenode1 namenode1-->
        <name>dfs.ha.namenodes.namenodes</name>
        <value>namenode1,namenode2</value>
    </property>
    <property>
        <!--配置的是具体的 namenode1 RPC 地址 ,注意下面的 name 值要和上面的对应层级关系-->
        <name>dfs.namenode.rpc-address.namenodes.namenode1</name>
        <value>hadoop1:8020</value>
    </property>
    <property>
        <!--配置 namenode2 RPC 地址-->
        <name>dfs.namenode.rpc-address.namenodes.namenode2</name>
        <value>hadoop2:8020</value>
    </property>
    <property>
        <!--配置 namenode1 的 http 地址-->
        <name>dfs.namenode.http-address.namenodes.namenode1</name>
        <value>hadoop1:50070</value>
    </property>
    <property>
        <!--配置 namenode2 的 http 地址-->
        <name>dfs.namenode.http-address.namenodes.namenode2</name>
        <value>hadoop2:50070</value>
    </property>
    <property>
        <!-- 指定 NameNode 的元数据在 JournalNode 上的存放位置 同时也是指定了 JournalNode -->
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://hadoop2:8485;hadoop3:8485;hadoop4:8485/namenodes
        </value>
    </property>
    <property>
        <!-- 配置 client 的 failover 代理配置 -->
        <name>dfs.client.failover.proxy.provider.namenodes</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <!-- 配置隔离机制，如果ssh是默认22端口，value直接写sshfence即可 -->
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <!-- 使用隔离机制时需要 ssh 免登陆 -->
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_dsa</value>
    </property>
    <property>
        <!-- 指定journalnode日志文件存储的路径 -->
        <name>dfs.journalnode.edits.dir</name>
        <value>/opt/journalnodeData</value>
    </property>
    <property>
        <!-- 开启自动故障转移 -->
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
</configuration>

```

---

##### 配置安装 zookepper   

==作为 zookeeper 服务器的节点 就需要 zookeeper==

在需要的运行 zookeeper 的服务器上，安装 zookeeper （就是解压 配置环境变量）

然后编辑配置文件 zookeeper/conf/zoo.cfg  如果没有就用 zoo_sample.cfg 做模板

配置好了后 可以把 这个 zoo.cfg 文件发给其他服务器

==不能同时有 zoo.cfg 和 zoo_sample.cfg==

###### zoo.cfg

```ini

# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
# 这里是配置 zookeeper 数据目录
dataDir=/opt/zookeeperData
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1

# 这里配置的是 3 台 zookeeper 服务器
server.1=hadoop1:2888:3888
server.2=hadoop2:2888:3888
server.3=hadoop3:2888:3888
```

然后在每台服务器的 dataDir 目录下面 创建一个 myid 文件，文件内容每台服务器分别是 1   2   3

1   2  3 对应着 zoo.cfg 文件里面的设置

安装好了后 在 3 个服务器上启动 zookeeper `zkServer.sh start`  

> 查看状态 `zkServer.sh status`

> 停止 `zkServer.sh stop`

---

#### Yarn 配置

Yarn 主要是配置 ResourceManager ，随便在哪些机器上面配置都行

###### mapred-site.xml  （每个服务器上都需要配置）

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        // 如果要使用 mapreduce 程序，那么就需要这样写，固定写法
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

###### yarn-site.xml  （每个服务器上都需要配置）

ResourceManager 和 NodeManager 主从结构 ，ResourceManager 存在单点故障问题  ，所以它也需要 ZK

```xml
<?xml version="1.0"?>
<configuration>
    <property>
        <!--NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序-->
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <!--是否开启 resourcemanager 高可用-->
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <!--这里配置的就是 HDFS 的名字，是在 hdfs-site.xml 里面指定的-->
        <name>yarn.resourcemanager.cluster-id</name>
        <value>namenodes</value>
    </property>
    <property>
        <!--这里是定义好两个 resourcemanager-->
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <!--这里是具体的 rm1-->
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>hadoop3</value>
    </property>
    <property>
        <!--这里是具体的 rm2-->
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>hadoop4</value>
    </property>
    <property>
        <!--这里是指定我们之前的 zk 地址-->
        <name>yarn.resourcemanager.zk-address</name>
        <value>hadoop1:2181,hadoop2:2181,hadoop3:2181</value>
    </property>
    <property>
        # 这个在 Spark on Yarn with client 的时候，需要设置，不然 spark 会报错
      	# 虚拟内存设置是否生效，如果为 True，那么那么当运行任务时，虚拟内存超过了，默认值，那么container会被杀死，如果为 False ，就不会被杀死
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>
```



##### 初始化各个组件

- 启动所有的 zkServer `zkServer.sh start`
- 启动所有的 journalnode `hadoop-daemon.sh start journalnode`

- 在其中一个 NameNode 上格式化：`hdfs namenode -format`
- 启动刚刚格式化的 NameNode : `hadoop-daemon.sh start namenode`
- 在没有格式化的 NameNode 上执行：`hdfs namenode -bootstrapStandby`
- 启动第二个 NameNode ： `hadoop-daemon.sh start namenode`
- 在其中一个 NameNode 上初始化 zkfc ( zkfc 不需要我们配置，自动配置的) : `hdfs zkfc -formatZK`
- 停止上面的节点 `stop-dfs.sh`



##### 使用

1. 在部署了zkserver 的服务器上启动 zkserver： `zkServer.sh start `  ( 每台都要 )
2. 在任意一台 NameNode 上启动 HDFS : `start-dfs.sh`
3. 在任意一台机器上启动 Yarn ：`start-yarn.sh` ，这里会自动启动 NodeManager
4. 在部署了 ResourceManager 的机器上启动 ResourceManager : `yarn-daemon.sh start resourcemanager`( 每台都要 )

##### 关闭

1. `stop-all.sh`
2. `yarn-daemon.sh stop resourcemanager`
3. `zkServer.sh stop`

 

Namenode 和 ResourceManger 如果不是同一台机器，不能通过 `start-yarn.sh`上启动 ResouceManager ，应该在 ResouceManager 所在的机器上启动 ResouceManager `yarn-daemon.sh start resoucemanager `



==注意==

如果有问题想要重新初始化，那么我们可以

- 先把所有的节点的进程关闭 `killalll java`
- 删除之前格式化的数据目录hadoop.tmp.dir属性对应的目录 
- 所有节点同步都删掉，别单删掉之前的一个，删掉三台JN节点中dfs.journalnode.edits.dir属性所对应的目录

接着就重新格式化 NameNode 

---

#### 关于启动命令



##### 首先需要明白一点，我们部署的服务（框架）都需要启动

##### 命令解释

一、`zkServer.sh`（在每一个 zoookeeper 节点上执行）的作用是启动==当前==节点上的 `zookeeper`，

 执行了`start-all.sh` 相当于执行 `start-dfs.sh`和`start-yarn.sh`



二、`start-dfs.sh`(在部署了 NameNode 的节点上执行，执行一次)的作用是启动所有节点上的 `NameNode`以及所有节点上的`DataNode`，以及 `Journal Node`  ， `ZKFC。`



上面几步，我们就启动了 `ZK`   `ZKFC`   `NameNode`   `DataNode`   `Journal Node`。这些服务我们都可以通过

`hadoop-daemon.sh start 服务名` 单独启动，`start-all.sh` 实际上也是调用了 `hadoop-daemon.sh start 服务名`。

比如 `hadoop-daemon.sh start namenode` `hadoop-daemon.sh start datanode` `hadoop-daemon.sh start journalnode`

当然我们使用 `hadoop-daemon.sh start 服务名`启动的时候，需要在每一台部署了这个服务的节点上执行。

由于`start-dfs.sh` 会自动关联其他的节点，并启动其他节点上的服务，所以只需要执行一次。



执行了`start-yarn.sh`相当于执行了 `yarn-damon.sh start 服务名`

三、上面的步骤，我们就启动了`ZK`   `ZKFC`   `NameNode`   `DataNode`   `Journal Node`。

接下来，我们还需要启动 `resourcemanager` 和 `nodemanager` ，`nodemanager` 可以关联启动（关联启动，就是在一个节点上执行某个命令，其他节点上的服务也会启动），我们可以使用 `start-yarn.sh`来关联启动 `nodemanager`,

我们只需要在部署了 `DataNode` 的节点上面执行 `start-yarn.sh`(任意一台执行就行)就可以（因为`nodemanager` 在 `DataNode`上面）

执行了 `start-yarn.sh` 后，所有的 `nodemanager`就启动了。实际上 `start-yarn.sh`在每一台部署了`nodemanager`的节点上面执行`yarn-daemon.sh start nodemanager`，同时会在 当前节点执行 `yarn-daemon.sh start resourcemanager`

如果当前节点有 `resourcemanager`，那么这个 `resourcemanager`就启动成功，这样我们只需要在另外一个节点上面启动 `resourcemanager` 。

如果当前节点没有 `resourcemanager` 那么我们就需要在其他两个节点上面执行  `yarn-daemon.sh start resourcemanager`



##### 总结

不管用什么方法，是单独启动，还是关联启动，只要把我们需要的服务都启动了，就没问题。

有一种方法一定可以成功，就是在所有的节点上面都执行 `start-all.sh`，即使某些节点上面没有相应的服务，也不要紧,这样一定会成功.

